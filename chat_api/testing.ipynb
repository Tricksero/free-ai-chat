{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gpt4all_chat\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fertig\n"
     ]
    }
   ],
   "source": [
    "save_path = gpt4all_chat.get_save_path()\n",
    "local_models = os.listdir(save_path)\n",
    "for model in gpt4all_chat.get_list_of_all_models():\n",
    "    if not model[\"filename\"] in local_models:\n",
    "        print(f\"model:' {model['filename']}' is missing\")\n",
    "print(\"fertig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wizardlm-13b-v1.1-superhot-8k.ggmlv3.q4_0.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|â–ˆ         | 748M/7.32G [01:51<16:24, 6.68MiB/s] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/gpt4all/chat_api/testing.ipynb Cell 3\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6c6f63616c2d67707434616c6c2d776562736974652d7765622d31227d/gpt4all/chat_api/testing.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mif\u001b[39;00m m[\u001b[39m\"\u001b[39m\u001b[39mfilename\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m gpt4all_chat\u001b[39m.\u001b[39mlocal_models():\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6c6f63616c2d67707434616c6c2d776562736974652d7765622d31227d/gpt4all/chat_api/testing.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39mprint\u001b[39m(m[\u001b[39m\"\u001b[39m\u001b[39mfilename\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m----> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6c6f63616c2d67707434616c6c2d776562736974652d7765622d31227d/gpt4all/chat_api/testing.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     gpt4all_chat\u001b[39m.\u001b[39;49mChat(m[\u001b[39m\"\u001b[39;49m\u001b[39mfilename\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6c6f63616c2d67707434616c6c2d776562736974652d7765622d31227d/gpt4all/chat_api/testing.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mFertig\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/gpt4all/chat_api/gpt4all_chat.py:73\u001b[0m, in \u001b[0;36mChat.__init__\u001b[0;34m(self, model_name, token)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, model_name: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mllama-2-7b-chat.ggmlv3.q4_0.bin\u001b[39m\u001b[39m\"\u001b[39m, token \u001b[39m=\u001b[39m \u001b[39m200\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_name \u001b[39m=\u001b[39m model_name\n\u001b[0;32m---> 73\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m GPT4ALL(model_name)\n\u001b[1;32m     74\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtoken \u001b[39m=\u001b[39m token\n\u001b[1;32m     75\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mchat_session()\n",
      "File \u001b[0;32m/gpt4all/chat_api/gpt4all_chat.py:49\u001b[0m, in \u001b[0;36mGPT4ALL.__init__\u001b[0;34m(self, model_name, model_path, model_type, allow_download, n_threads)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, model_name: \u001b[39mstr\u001b[39m,\n\u001b[1;32m     27\u001b[0m              model_path: \u001b[39mstr\u001b[39m \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     28\u001b[0m              model_type: \u001b[39mstr\u001b[39m \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     29\u001b[0m              allow_download: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m     30\u001b[0m              n_threads: \u001b[39mint\u001b[39m \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m):\n\u001b[1;32m     31\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[39m    init the class.\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39m    return None\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(model_name, model_path, model_type, allow_download, n_threads)\n",
      "File \u001b[0;32m/gpt4all/venv/lib/python3.11/site-packages/gpt4all/gpt4all.py:87\u001b[0m, in \u001b[0;36mGPT4All.__init__\u001b[0;34m(self, model_name, model_path, model_type, allow_download, n_threads)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m pyllmodel\u001b[39m.\u001b[39mLLModel()\n\u001b[1;32m     86\u001b[0m \u001b[39m# Retrieve model and download if allowed\u001b[39;00m\n\u001b[0;32m---> 87\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig: ConfigType \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mretrieve_model(\n\u001b[1;32m     88\u001b[0m     model_name, model_path\u001b[39m=\u001b[39;49mmodel_path, allow_download\u001b[39m=\u001b[39;49mallow_download\n\u001b[1;32m     89\u001b[0m )\n\u001b[1;32m     90\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mload_model(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig[\u001b[39m\"\u001b[39m\u001b[39mpath\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m     91\u001b[0m \u001b[39m# Set n_threads\u001b[39;00m\n",
      "File \u001b[0;32m/gpt4all/venv/lib/python3.11/site-packages/gpt4all/gpt4all.py:173\u001b[0m, in \u001b[0;36mGPT4All.retrieve_model\u001b[0;34m(model_name, model_path, allow_download, verbose)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[39melif\u001b[39;00m allow_download:\n\u001b[1;32m    171\u001b[0m     url \u001b[39m=\u001b[39m config\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39murl\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m--> 173\u001b[0m     config[\u001b[39m\"\u001b[39m\u001b[39mpath\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m GPT4All\u001b[39m.\u001b[39;49mdownload_model(\n\u001b[1;32m    174\u001b[0m         model_filename, model_path, verbose\u001b[39m=\u001b[39;49mverbose, url\u001b[39m=\u001b[39;49murl\n\u001b[1;32m    175\u001b[0m     )\n\u001b[1;32m    176\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mFailed to retrieve model\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/gpt4all/venv/lib/python3.11/site-packages/gpt4all/gpt4all.py:217\u001b[0m, in \u001b[0;36mGPT4All.download_model\u001b[0;34m(model_filename, model_path, verbose, url)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    216\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(download_path, \u001b[39m\"\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m file:\n\u001b[0;32m--> 217\u001b[0m         \u001b[39mfor\u001b[39;00m data \u001b[39min\u001b[39;00m response\u001b[39m.\u001b[39miter_content(block_size):\n\u001b[1;32m    218\u001b[0m             progress_bar\u001b[39m.\u001b[39mupdate(\u001b[39mlen\u001b[39m(data))\n\u001b[1;32m    219\u001b[0m             file\u001b[39m.\u001b[39mwrite(data)\n",
      "File \u001b[0;32m/gpt4all/venv/lib/python3.11/site-packages/requests/models.py:816\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    814\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw, \u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    815\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 816\u001b[0m         \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw\u001b[39m.\u001b[39mstream(chunk_size, decode_content\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    817\u001b[0m     \u001b[39mexcept\u001b[39;00m ProtocolError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    818\u001b[0m         \u001b[39mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[0;32m/gpt4all/venv/lib/python3.11/site-packages/urllib3/response.py:940\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    938\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    939\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m is_fp_closed(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp) \u001b[39mor\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decoded_buffer) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 940\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(amt\u001b[39m=\u001b[39;49mamt, decode_content\u001b[39m=\u001b[39;49mdecode_content)\n\u001b[1;32m    942\u001b[0m         \u001b[39mif\u001b[39;00m data:\n\u001b[1;32m    943\u001b[0m             \u001b[39myield\u001b[39;00m data\n",
      "File \u001b[0;32m/gpt4all/venv/lib/python3.11/site-packages/urllib3/response.py:879\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    876\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decoded_buffer) \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m amt:\n\u001b[1;32m    877\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decoded_buffer\u001b[39m.\u001b[39mget(amt)\n\u001b[0;32m--> 879\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_raw_read(amt)\n\u001b[1;32m    881\u001b[0m flush_decoder \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    882\u001b[0m \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/gpt4all/venv/lib/python3.11/site-packages/urllib3/response.py:814\u001b[0m, in \u001b[0;36mHTTPResponse._raw_read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    811\u001b[0m fp_closed \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp, \u001b[39m\"\u001b[39m\u001b[39mclosed\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    813\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_error_catcher():\n\u001b[0;32m--> 814\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fp_read(amt) \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m fp_closed \u001b[39melse\u001b[39;00m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    815\u001b[0m     \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m amt \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m data:\n\u001b[1;32m    816\u001b[0m         \u001b[39m# Platform-specific: Buggy versions of Python.\u001b[39;00m\n\u001b[1;32m    817\u001b[0m         \u001b[39m# Close the connection when no data is returned\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    822\u001b[0m         \u001b[39m# not properly close the connection in all cases. There is\u001b[39;00m\n\u001b[1;32m    823\u001b[0m         \u001b[39m# no harm in redundantly calling close.\u001b[39;00m\n\u001b[1;32m    824\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m/gpt4all/venv/lib/python3.11/site-packages/urllib3/response.py:799\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    796\u001b[0m     \u001b[39mreturn\u001b[39;00m buffer\u001b[39m.\u001b[39mgetvalue()\n\u001b[1;32m    797\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    798\u001b[0m     \u001b[39m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[0;32m--> 799\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fp\u001b[39m.\u001b[39;49mread(amt) \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp\u001b[39m.\u001b[39mread()\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/http/client.py:466\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m amt \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength:\n\u001b[1;32m    464\u001b[0m     \u001b[39m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[1;32m    465\u001b[0m     amt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength\n\u001b[0;32m--> 466\u001b[0m s \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfp\u001b[39m.\u001b[39mread(amt)\n\u001b[1;32m    467\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m s \u001b[39mand\u001b[39;00m amt:\n\u001b[1;32m    468\u001b[0m     \u001b[39m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[1;32m    469\u001b[0m     \u001b[39m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[1;32m    470\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_close_conn()\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    705\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 706\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    707\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[1;32m    708\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/ssl.py:1278\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1274\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1275\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1276\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m   1277\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[0;32m-> 1278\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[1;32m   1279\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1280\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/ssl.py:1134\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1132\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1133\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1134\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[1;32m   1135\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1136\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for m in gpt4all_chat.get_list_of_all_models():\n",
    "    if m[\"filename\"] not in gpt4all_chat.local_models():\n",
    "        print(m[\"filename\"])\n",
    "        gpt4all_chat.Chat(m[\"filename\"])\n",
    "        print(\"Fertig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: 0/5\n",
      "model: 1/5\n",
      "model: 2/5\n",
      "model: 3/5\n",
      "model: 4/5\n",
      "Found model file at  /root/.cache/gpt4all/wizardlm-13b-v1.1-superhot-8k.ggmlv3.q4_0.bin\n",
      "The model: wizardlm-13b-v1.1-superhot-8k.ggmlv3.q4_0.bin\n",
      " answer: \n",
      "fertig\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from /root/.cache/gpt4all/wizardlm-13b-v1.1-superhot-8k.ggmlv3.q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32001\n",
      "llama_model_load_internal: n_ctx      = 2048\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size =    0.00 MB\n",
      "error loading model: llama.cpp: tensor 'layers.1.attention_norm.weight' is missing from model\n",
      "llama_load_model_from_file: failed to load model\n",
      "LLAMA ERROR: failed to load model from /root/.cache/gpt4all/wizardlm-13b-v1.1-superhot-8k.ggmlv3.q4_0.bin\n",
      "LLaMA ERROR: prompt won't work with an unloaded model!\n"
     ]
    }
   ],
   "source": [
    "models_list = gpt4all_chat.local_models()\n",
    "for model in models_list:\n",
    "    print(f\"model: {models_list.index(model)}/{len(models_list)}\")\n",
    "    if model in [\"ggml-all-MiniLM-L6-v2-f16.bin\", \"starcoderbase-3b-ggml.bin\", \"llama-2-7b-chat.ggmlv3.q4_0.bin\",\"nous-hermes-13b.ggmlv3.q4_0.bin\"]:\n",
    "        continue\n",
    "    chat = gpt4all_chat.Chat(model)\n",
    "    response = chat.new_message(\"What is the different between GPU and CPU for AI training?\")\n",
    "    print(f\"The model: {model}\\n answer: {response}\")\n",
    "print(\"fertig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22:49:30.352918 lo    In  IP (tos 0x0, ttl 64, id 59835, offset 0, flags [DF], proto TCP (6), length 194)\n",
      "\n",
      "    localhost.40489 > localhost.50814: Flags [P.], cksum 0xfeb6 (incorrect -> 0x8b03), seq 3055674865:3055675007, ack 915228316, win 512, options [nop,nop,TS val 2325485205 ecr 2325485160], length 142\n",
      "\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n"
     ]
    }
   ],
   "source": [
    "with open(\"./log_before.log\", \"r\") as f:\n",
    "    before = f.readlines()\n",
    "\n",
    "with open(\"./log_while.log\", \"r\") as f:\n",
    "    after = f.readlines()\n",
    "print(after[0])\n",
    "print(after[1])\n",
    "# for element in after:\n",
    "#     if not element in before:\n",
    "#         with open(\"new_trafic.log\", \"a\") as f:\n",
    "#             f.write(element)\n",
    "\n",
    "# for i in range(1, len(after), 2):\n",
    "#     if after[i].replace(\"127.0.0.11\", \"localhost\").count(\"localhost\") != 2:\n",
    "#         with open(\"new_trafic2.log\", \"a\") as f:\n",
    "#             f.write(after[i-1])\n",
    "#             f.write(after[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nOk erst einmal alle models durch rechnen lassen und danach nochmal den netzwerk verkehr prÃ¼fen. Kann ja auch ein Download sein ich kann es nicht nachweisen.\\n\\noutput erster versuch:\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"gpt4all.io = 172.67.71.169\"\n",
    "\"\"\"\n",
    "Ok erst einmal alle models durch rechnen lassen und danach nochmal den netzwerk verkehr prÃ¼fen. Kann ja auch ein Download sein ich kann es nicht nachweisen.\n",
    "\n",
    "output erster versuch:\n",
    "model: 0/13\n",
    "Found model file at  /root/.cache/gpt4all/ggml-all-MiniLM-L6-v2-f16.bin\n",
    "The model: ggml-all-MiniLM-L6-v2-f16.bin\n",
    " answer: !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "model: 1/13\n",
    "Found model file at  /root/.cache/gpt4all/starcoderbase-3b-ggml.bin\n",
    "starcoder_model_load: loading model from '/root/.cache/gpt4all/starcoderbase-3b-ggml.bin'\n",
    "starcoder_model_load: n_vocab = 49152\n",
    "starcoder_model_load: n_ctx   = 8192\n",
    "starcoder_model_load: n_embd  = 2816\n",
    "starcoder_model_load: n_head  = 22\n",
    "starcoder_model_load: n_layer = 36\n",
    "starcoder_model_load: ftype   = 1\n",
    "starcoder_model_load: qntvr   = 0\n",
    "starcoder_model_load: ggml ctx size = 7155.26 MB\n",
    "starcoder_model_load: memory_size =  6336.00 MB, n_mem = 294912\n",
    "starcoder_model_load: model size  =  3742.60 MB\n",
    "The model: starcoderbase-3b-ggml.bin\n",
    " answer: <fim_pad><commit_msg>4*0&*<reponame>4<reponame>*&0(<jupyter_code>*$<commit_msg>.<commit_msg>50*.0.$4*4(5*&$\"&<commit_msg>45<fim_pad>$<fim_pad>\"<fim_pad><issue_comment>*<commit_msg>4$(0(&4*<jupyter_start><issue_comment>5$45<issue_comment>*($4<reponame><fim_pad>225\"2\"(*<jupyter_start>54$(<commit_msg><reponame><issue_comment>\"\"5\"2*4(2.<reponame>($<reponame><reponame>5*<fim_pad>$<fim_pad>&&(<jupyter_start><issue_comment>*<fim_pad>0<jupyter_code>(<issue_closed>54$<jupyter_start>$*$<fim_pad>\"\"<fim_pad>&<reponame><reponame><jupyter_start>*4$<commit_msg><jupyter_start>*254<commit_msg><jupyter_code><jupyter_start>\"(2<issue_comment>&<jupyter_code>0<jupyter_code><commit_msg><fim_pad>\"02<reponame><reponame><commit_msg><jupyter_start>*<reponame><issue_comment>2<commit_msg><reponame>(<jupyter_code><issue_comment><issue_comment>2<commit_msg><issue_comment>&&&5220<reponame><jupyter_start>\"<reponame><issue_comment>&\"<fim_pad>(*<reponame>5<jupyter_code><fim_pad><issue_comment><commit_msg>\"<fim_pad><fim_pad>5\n",
    "model: 2/13\n",
    "Found model file at  /root/.cache/gpt4all/ggml-replit-code-v1-3b.bin\n",
    "Invalid model file\n",
    "\n",
    "\n",
    "2. versuch:\n",
    "\n",
    "model: 2/10\n",
    "Found model file at  /root/.cache/gpt4all/llama-2-7b-chat.ggmlv3.q4_0.bin\n",
    "llama.cpp: loading model from /root/.cache/gpt4all/llama-2-7b-chat.ggmlv3.q4_0.bin\n",
    "llama_model_load_internal: format     = ggjt v3 (latest)\n",
    "llama_model_load_internal: n_vocab    = 32000\n",
    "llama_model_load_internal: n_ctx      = 2048\n",
    "llama_model_load_internal: n_embd     = 4096\n",
    "llama_model_load_internal: n_mult     = 256\n",
    "llama_model_load_internal: n_head     = 32\n",
    "llama_model_load_internal: n_layer    = 32\n",
    "llama_model_load_internal: n_rot      = 128\n",
    "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
    "llama_model_load_internal: n_ff       = 11008\n",
    "llama_model_load_internal: n_parts    = 1\n",
    "llama_model_load_internal: model size = 7B\n",
    "llama_model_load_internal: ggml ctx size =    0.07 MB\n",
    "llama_model_load_internal: mem required  = 5407.71 MB (+ 1026.00 MB per state)\n",
    "llama_new_context_with_model: kv self size  = 1024.00 MB\n",
    "The model: llama-2-7b-chat.ggmlv3.q4_0.bin\n",
    " answer:\n",
    " Unterscheidung Between GPU And CPU For AI Training 2023?\n",
    "\n",
    "GPUs (Graphics Processing Units) are specialized computer chips designed specifically for handling complex mathematical computations at high speeds. They have become an essential component of modern computing, particularly in the field of artificial intelligence (AI). In this article, we will explore the differences between GPU and CPU for AI training, including their strengths, weaknesses, and use cases.\n",
    "GPU vs CPU: What's the Difference?\n",
    "Before diving into the specifics of GPU vs CPU for AI training, it's important to understand the basics of these two types of processors.\n",
    "CPU (Central Processing Unit): The central processing unit is the \"brain\" of a computer, responsible for executing most instructions that a program can execute. It performs calculations and makes decisions based on those calculations. CPUs are designed to handle general-pur\n",
    "model: 3/10\n",
    "Found model file at  /root/.cache/gpt4all/nous-hermes-13b.ggmlv3.q4_0.bin\n",
    "llama.cpp: loading model from /root/.cache/gpt4all/nous-hermes-13b.ggmlv3.q4_0.bin\n",
    "llama_model_load_internal: format     = ggjt v3 (latest)\n",
    "llama_model_load_internal: n_vocab    = 32001\n",
    "llama_model_load_internal: n_ctx      = 2048\n",
    "llama_model_load_internal: n_embd     = 5120\n",
    "llama_model_load_internal: n_mult     = 256\n",
    "llama_model_load_internal: n_head     = 40\n",
    "llama_model_load_internal: n_layer    = 40\n",
    "llama_model_load_internal: n_rot      = 128\n",
    "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
    "llama_model_load_internal: n_ff       = 13824\n",
    "llama_model_load_internal: n_parts    = 1\n",
    "llama_model_load_internal: model size = 13B\n",
    "llama_model_load_internal: ggml ctx size =    0.07 MB\n",
    "error loading model: llama.cpp: tensor 'layers.33.ffn_norm.weight' is missing from model\n",
    "llama_load_model_from_file: failed to load model\n",
    "LLAMA ERROR: failed to load model from /root/.cache/gpt4all/nous-hermes-13b.ggmlv3.q4_0.bin\n",
    "The model: nous-hermes-13b.ggmlv3.q4_0.bin\n",
    " answer:\n",
    "model: 4/10\n",
    "LLaMA ERROR: prompt won't work with an unloaded model!\n",
    "Found model file at  /root/.cache/gpt4all/ggml-model-gpt4all-falcon-q4_0.bin\n",
    "Invalid model file\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
