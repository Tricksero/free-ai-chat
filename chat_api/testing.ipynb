{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gpt4all_chat\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model:' mistral-7b-openorca.Q4_0.gguf' is missing\n",
      "model:' mistral-7b-instruct-v0.1.Q4_0.gguf' is missing\n",
      "model:' gpt4all-falcon-q4_0.gguf' is missing\n",
      "model:' orca-2-7b.Q4_0.gguf' is missing\n",
      "model:' orca-2-13b.Q4_0.gguf' is missing\n",
      "model:' wizardlm-13b-v1.2.Q4_0.gguf' is missing\n",
      "model:' nous-hermes-llama2-13b.Q4_0.gguf' is missing\n",
      "model:' gpt4all-13b-snoozy-q4_0.gguf' is missing\n",
      "model:' mpt-7b-chat-merges-q4_0.gguf' is missing\n",
      "model:' orca-mini-3b-gguf2-q4_0.gguf' is missing\n",
      "model:' replit-code-v1_5-3b-q4_0.gguf' is missing\n",
      "model:' starcoder-q4_0.gguf' is missing\n",
      "model:' rift-coder-v0-7b-q4_0.gguf' is missing\n",
      "model:' all-MiniLM-L6-v2-f16.gguf' is missing\n",
      "model:' em_german_mistral_v01.Q4_0.gguf' is missing\n",
      "fertig\n"
     ]
    }
   ],
   "source": [
    "save_path = gpt4all_chat.get_save_path()\n",
    "local_models = os.listdir(save_path)\n",
    "for model in gpt4all_chat.get_list_of_all_models():\n",
    "    if not model[\"filename\"] in local_models:\n",
    "        print(f\"model:' {model['filename']}' is missing\")\n",
    "print(\"fertig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mistral-7b-openorca.Q4_0.gguf\n",
      "\n",
      "\n",
      "Fertig\n"
     ]
    }
   ],
   "source": [
    "# gpt4all_chat.download_all_models()\n",
    "for m in gpt4all_chat.get_list_of_all_models():\n",
    "    # if m[\"filename\"] not in gpt4all_chat.local_models():\n",
    "        print(m[\"filename\"])\n",
    "        chat = gpt4all_chat.Chat(m[\"filename\"])\n",
    "        print(chat.new_message(\"write hello world in 3 different programming languages\"))\n",
    "        print(\"Fertig\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: 0/13\n",
      "Found model file at  /home/rwat12luj/.cache/gpt4all/GPT4All-13B-snoozy.ggmlv3.q4_0.bin\n",
      "Unable to instantiate model\n",
      "model: 1/13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Invalid model file\n",
      "Invalid model file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found model file at  /home/rwat12luj/.cache/gpt4all/ggml-all-MiniLM-L6-v2-f16.bin\n",
      "Unable to instantiate model\n",
      "model: 2/13\n",
      "Found model file at  /home/rwat12luj/.cache/gpt4all/ggml-model-gpt4all-falcon-q4_0.bin\n",
      "Unable to instantiate model\n",
      "model: 3/13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Invalid model file\n",
      "Invalid model file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found model file at  /home/rwat12luj/.cache/gpt4all/starcoderbase-3b-ggml.bin\n",
      "Unable to instantiate model\n",
      "model: 4/13\n",
      "Found model file at  /home/rwat12luj/.cache/gpt4all/starcoderbase-7b-ggml.bin\n",
      "Unable to instantiate model\n",
      "model: 5/13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Invalid model file\n",
      "Invalid model file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found model file at  /home/rwat12luj/.cache/gpt4all/ggml-replit-code-v1-3b.bin\n",
      "Unable to instantiate model\n",
      "model: 6/13\n",
      "Found model file at  /home/rwat12luj/.cache/gpt4all/orca-mini-3b.ggmlv3.q4_0.bin\n",
      "Unable to instantiate model\n",
      "model: 7/13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Invalid model file\n",
      "Invalid model file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found model file at  /home/rwat12luj/.cache/gpt4all/wizardLM-13B-Uncensored.ggmlv3.q4_0.bin\n",
      "Unable to instantiate model\n",
      "model: 8/13\n",
      "Found model file at  /home/rwat12luj/.cache/gpt4all/orca-mini-7b.ggmlv3.q4_0.bin\n",
      "Unable to instantiate model\n",
      "model: 9/13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Invalid model file\n",
      "Invalid model file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found model file at  /home/rwat12luj/.cache/gpt4all/wizardlm-13b-v1.1-superhot-8k.ggmlv3.q4_0.bin\n",
      "Unable to instantiate model\n",
      "model: 10/13\n",
      "Found model file at  /home/rwat12luj/.cache/gpt4all/llama-2-7b-chat.ggmlv3.q4_0.bin\n",
      "Unable to instantiate model\n",
      "model: 11/13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Invalid model file\n",
      "Invalid model file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found model file at  /home/rwat12luj/.cache/gpt4all/nous-hermes-13b.ggmlv3.q4_0.bin\n",
      "Unable to instantiate model\n",
      "model: 12/13\n",
      "Found model file at  /home/rwat12luj/.cache/gpt4all/orca-mini-13b.ggmlv3.q4_0.bin\n",
      "Unable to instantiate model\n",
      "fertig\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Invalid model file\n"
     ]
    }
   ],
   "source": [
    "models_list = gpt4all_chat.local_models()\n",
    "problems = []\n",
    "for model in models_list:\n",
    "    print(f\"model: {models_list.index(model)}/{len(models_list)}\")\n",
    "    # if model in [\"ggml-all-MiniLM-L6-v2-f16.bin\", \"starcoderbase-3b-ggml.bin\", \"llama-2-7b-chat.ggmlv3.q4_0.bin\",\"nous-hermes-13b.ggmlv3.q4_0.bin\"]:\n",
    "    #     continue\n",
    "    try:\n",
    "        chat = gpt4all_chat.Chat(model)\n",
    "        response = chat.new_message(\"What is the different between GPU and CPU for AI training?\")\n",
    "        print(f\"The model: {model}\\n answer: {response}\")\n",
    "    except Exception as e:\n",
    "        print(\"error:\")\n",
    "        print(e)\n",
    "        problems.append(model)\n",
    "print(\"fertig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['GPT4All-13B-snoozy.ggmlv3.q4_0.bin', 'ggml-all-MiniLM-L6-v2-f16.bin', 'ggml-model-gpt4all-falcon-q4_0.bin', 'starcoderbase-3b-ggml.bin', 'starcoderbase-7b-ggml.bin', 'ggml-replit-code-v1-3b.bin', 'orca-mini-3b.ggmlv3.q4_0.bin', 'wizardLM-13B-Uncensored.ggmlv3.q4_0.bin', 'orca-mini-7b.ggmlv3.q4_0.bin', 'wizardlm-13b-v1.1-superhot-8k.ggmlv3.q4_0.bin', 'llama-2-7b-chat.ggmlv3.q4_0.bin', 'nous-hermes-13b.ggmlv3.q4_0.bin', 'orca-mini-13b.ggmlv3.q4_0.bin']\n",
      "13\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "print(problems)\n",
    "print(len(problems))\n",
    "print(os.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22:49:30.352918 lo    In  IP (tos 0x0, ttl 64, id 59835, offset 0, flags [DF], proto TCP (6), length 194)\n",
      "\n",
      "    localhost.40489 > localhost.50814: Flags [P.], cksum 0xfeb6 (incorrect -> 0x8b03), seq 3055674865:3055675007, ack 915228316, win 512, options [nop,nop,TS val 2325485205 ecr 2325485160], length 142\n",
      "\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n",
      "aaa\n"
     ]
    }
   ],
   "source": [
    "with open(\"./log_before.log\", \"r\") as f:\n",
    "    before = f.readlines()\n",
    "\n",
    "with open(\"./log_while.log\", \"r\") as f:\n",
    "    after = f.readlines()\n",
    "print(after[0])\n",
    "print(after[1])\n",
    "# for element in after:\n",
    "#     if not element in before:\n",
    "#         with open(\"new_trafic.log\", \"a\") as f:\n",
    "#             f.write(element)\n",
    "\n",
    "# for i in range(1, len(after), 2):\n",
    "#     if after[i].replace(\"127.0.0.11\", \"localhost\").count(\"localhost\") != 2:\n",
    "#         with open(\"new_trafic2.log\", \"a\") as f:\n",
    "#             f.write(after[i-1])\n",
    "#             f.write(after[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nOk erst einmal alle models durch rechnen lassen und danach nochmal den netzwerk verkehr prüfen. Kann ja auch ein Download sein ich kann es nicht nachweisen.\\n\\noutput erster versuch:\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"gpt4all.io = 172.67.71.169\"\n",
    "\"\"\"\n",
    "Ok erst einmal alle models durch rechnen lassen und danach nochmal den netzwerk verkehr prüfen. Kann ja auch ein Download sein ich kann es nicht nachweisen.\n",
    "\n",
    "output erster versuch:\n",
    "model: 0/13\n",
    "Found model file at  /root/.cache/gpt4all/ggml-all-MiniLM-L6-v2-f16.bin\n",
    "The model: ggml-all-MiniLM-L6-v2-f16.bin\n",
    " answer: !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "model: 1/13\n",
    "Found model file at  /root/.cache/gpt4all/starcoderbase-3b-ggml.bin\n",
    "starcoder_model_load: loading model from '/root/.cache/gpt4all/starcoderbase-3b-ggml.bin'\n",
    "starcoder_model_load: n_vocab = 49152\n",
    "starcoder_model_load: n_ctx   = 8192\n",
    "starcoder_model_load: n_embd  = 2816\n",
    "starcoder_model_load: n_head  = 22\n",
    "starcoder_model_load: n_layer = 36\n",
    "starcoder_model_load: ftype   = 1\n",
    "starcoder_model_load: qntvr   = 0\n",
    "starcoder_model_load: ggml ctx size = 7155.26 MB\n",
    "starcoder_model_load: memory_size =  6336.00 MB, n_mem = 294912\n",
    "starcoder_model_load: model size  =  3742.60 MB\n",
    "The model: starcoderbase-3b-ggml.bin\n",
    " answer: <fim_pad><commit_msg>4*0&*<reponame>4<reponame>*&0(<jupyter_code>*$<commit_msg>.<commit_msg>50*.0.$4*4(5*&$\"&<commit_msg>45<fim_pad>$<fim_pad>\"<fim_pad><issue_comment>*<commit_msg>4$(0(&4*<jupyter_start><issue_comment>5$45<issue_comment>*($4<reponame><fim_pad>225\"2\"(*<jupyter_start>54$(<commit_msg><reponame><issue_comment>\"\"5\"2*4(2.<reponame>($<reponame><reponame>5*<fim_pad>$<fim_pad>&&(<jupyter_start><issue_comment>*<fim_pad>0<jupyter_code>(<issue_closed>54$<jupyter_start>$*$<fim_pad>\"\"<fim_pad>&<reponame><reponame><jupyter_start>*4$<commit_msg><jupyter_start>*254<commit_msg><jupyter_code><jupyter_start>\"(2<issue_comment>&<jupyter_code>0<jupyter_code><commit_msg><fim_pad>\"02<reponame><reponame><commit_msg><jupyter_start>*<reponame><issue_comment>2<commit_msg><reponame>(<jupyter_code><issue_comment><issue_comment>2<commit_msg><issue_comment>&&&5220<reponame><jupyter_start>\"<reponame><issue_comment>&\"<fim_pad>(*<reponame>5<jupyter_code><fim_pad><issue_comment><commit_msg>\"<fim_pad><fim_pad>5\n",
    "model: 2/13\n",
    "Found model file at  /root/.cache/gpt4all/ggml-replit-code-v1-3b.bin\n",
    "Invalid model file\n",
    "\n",
    "\n",
    "2. versuch:\n",
    "\n",
    "model: 2/10\n",
    "Found model file at  /root/.cache/gpt4all/llama-2-7b-chat.ggmlv3.q4_0.bin\n",
    "llama.cpp: loading model from /root/.cache/gpt4all/llama-2-7b-chat.ggmlv3.q4_0.bin\n",
    "llama_model_load_internal: format     = ggjt v3 (latest)\n",
    "llama_model_load_internal: n_vocab    = 32000\n",
    "llama_model_load_internal: n_ctx      = 2048\n",
    "llama_model_load_internal: n_embd     = 4096\n",
    "llama_model_load_internal: n_mult     = 256\n",
    "llama_model_load_internal: n_head     = 32\n",
    "llama_model_load_internal: n_layer    = 32\n",
    "llama_model_load_internal: n_rot      = 128\n",
    "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
    "llama_model_load_internal: n_ff       = 11008\n",
    "llama_model_load_internal: n_parts    = 1\n",
    "llama_model_load_internal: model size = 7B\n",
    "llama_model_load_internal: ggml ctx size =    0.07 MB\n",
    "llama_model_load_internal: mem required  = 5407.71 MB (+ 1026.00 MB per state)\n",
    "llama_new_context_with_model: kv self size  = 1024.00 MB\n",
    "The model: llama-2-7b-chat.ggmlv3.q4_0.bin\n",
    " answer:\n",
    " Unterscheidung Between GPU And CPU For AI Training 2023?\n",
    "\n",
    "GPUs (Graphics Processing Units) are specialized computer chips designed specifically for handling complex mathematical computations at high speeds. They have become an essential component of modern computing, particularly in the field of artificial intelligence (AI). In this article, we will explore the differences between GPU and CPU for AI training, including their strengths, weaknesses, and use cases.\n",
    "GPU vs CPU: What's the Difference?\n",
    "Before diving into the specifics of GPU vs CPU for AI training, it's important to understand the basics of these two types of processors.\n",
    "CPU (Central Processing Unit): The central processing unit is the \"brain\" of a computer, responsible for executing most instructions that a program can execute. It performs calculations and makes decisions based on those calculations. CPUs are designed to handle general-pur\n",
    "model: 3/10\n",
    "Found model file at  /root/.cache/gpt4all/nous-hermes-13b.ggmlv3.q4_0.bin\n",
    "llama.cpp: loading model from /root/.cache/gpt4all/nous-hermes-13b.ggmlv3.q4_0.bin\n",
    "llama_model_load_internal: format     = ggjt v3 (latest)\n",
    "llama_model_load_internal: n_vocab    = 32001\n",
    "llama_model_load_internal: n_ctx      = 2048\n",
    "llama_model_load_internal: n_embd     = 5120\n",
    "llama_model_load_internal: n_mult     = 256\n",
    "llama_model_load_internal: n_head     = 40\n",
    "llama_model_load_internal: n_layer    = 40\n",
    "llama_model_load_internal: n_rot      = 128\n",
    "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
    "llama_model_load_internal: n_ff       = 13824\n",
    "llama_model_load_internal: n_parts    = 1\n",
    "llama_model_load_internal: model size = 13B\n",
    "llama_model_load_internal: ggml ctx size =    0.07 MB\n",
    "error loading model: llama.cpp: tensor 'layers.33.ffn_norm.weight' is missing from model\n",
    "llama_load_model_from_file: failed to load model\n",
    "LLAMA ERROR: failed to load model from /root/.cache/gpt4all/nous-hermes-13b.ggmlv3.q4_0.bin\n",
    "The model: nous-hermes-13b.ggmlv3.q4_0.bin\n",
    " answer:\n",
    "model: 4/10\n",
    "LLaMA ERROR: prompt won't work with an unloaded model!\n",
    "Found model file at  /root/.cache/gpt4all/ggml-model-gpt4all-falcon-q4_0.bin\n",
    "Invalid model file\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Module:\n",
      "  body:\n",
      "    FunctionDef:\n",
      "      name:\n",
      "        add_numbers\n",
      "      args:\n",
      "        arguments:\n",
      "          posonlyargs:\n",
      "          args:\n",
      "            arg:\n",
      "              arg:\n",
      "                a\n",
      "              annotation:\n",
      "                None\n",
      "              type_comment:\n",
      "                None\n",
      "            arg:\n",
      "              arg:\n",
      "                b\n",
      "              annotation:\n",
      "                None\n",
      "              type_comment:\n",
      "                None\n",
      "          vararg:\n",
      "            None\n",
      "          kwonlyargs:\n",
      "          kw_defaults:\n",
      "          kwarg:\n",
      "            None\n",
      "          defaults:\n",
      "      body:\n",
      "        Assign:\n",
      "          targets:\n",
      "            Name:\n",
      "              id:\n",
      "                result\n",
      "              ctx:\n",
      "                Store:\n",
      "          value:\n",
      "            BinOp:\n",
      "              left:\n",
      "                Name:\n",
      "                  id:\n",
      "                    a\n",
      "                  ctx:\n",
      "                    Load:\n",
      "              op:\n",
      "                Add:\n",
      "              right:\n",
      "                Name:\n",
      "                  id:\n",
      "                    b\n",
      "                  ctx:\n",
      "                    Load:\n",
      "          type_comment:\n",
      "            None\n",
      "        Return:\n",
      "          value:\n",
      "            Name:\n",
      "              id:\n",
      "                result\n",
      "              ctx:\n",
      "                Load:\n",
      "      decorator_list:\n",
      "      returns:\n",
      "        None\n",
      "      type_comment:\n",
      "        None\n",
      "  type_ignores:\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
